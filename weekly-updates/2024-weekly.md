---
title: Weekly updates on large models
---
# Week-of-year:（2024-08-18 ~ 2024-08-24）
- VLP视觉大模型与视觉指令精挑
  
# Week-of-year: (2024-08-11 ~ 2024-08-17)
- LLM模型裁剪与知识萃取

从大的LLM更高效的得到小的LLM，目标分为深度裁剪（Layers）、宽度裁剪（attention heads，MLP中间层维度，embedding维度等）以及深度宽度同时裁剪三种情况，实现方案主要分依赖梯度计算和无梯度计算两类。Nvidia最近的工作[S. Muralidharan](https://arxiv.org/abs/2407.14679)，给出一个无梯度计算情况下深度和宽度同时裁剪的方案，该方案裁剪过程具有无梯度带来的低计算成本的优势，宽度上根据MHA、MLP及LN的Activation大小确定对应维度的重要性，深度上定义perplexity（移除一层计算perplexity的diff）和Block Importance（计算层输入/输出embedding的相似性）来估算层的重要性。把维度和层按照重要性排序后裁剪掉重要性低的维度，对得到的模型通过CPT来以原始模型的logits为目标进行萃取训练。训练后的模型继续在宽度和深度上做重要性评估和裁剪，再做CPT萃取训练，持续循环多次得到最终的裁剪模型。Apple的On-device端侧小模型（2.7B）的训练[AFM](https://machinelearning.apple.com/research/apple-intelligence-foundation-language-models)，也用初始化裁剪6.4B模型的方案得到，实现方案更多参考Sheared LLaMA[M. Xia](https://arxiv.org/abs/2310.06694)。Sheared LLaMA通过将裁剪问题转化为约束优化问题，支持在深度和宽度两个方向同时做裁剪。具体实现上，作者定义了一组取值[0,1]的pruning mask参数，对应于各Layer、MHA、MLP等结构的mask，将维度裁剪的要求转变为对pruning mask参数的约束，通过增广目标函数进行训练得到裁剪模型及初始权重。进一步，作者对裁剪模型初始模型做CPT，进一步回复裁剪模型的能力。CPT过程作者发现不同领域的数据，降低loss的程度不同，与分领域scaling law估算的loss下降不匹配，作者猜测与初始模型在不同领域的能力差异有关，因此提出了一个dynamic batch loading的思路，在训练过程中根据每次batch对不同领域loss下降的幅度，动态调整下个batch采样各领域数据的比例，给下降幅度多的领域更多采样权重，动态实现训练数据的最优配比。深度裁剪的工作如ShortGPT[X. Men](https://arxiv.org/abs/2403.03853)、Shortened LLaMA[B. Kim](https://arxiv.org/abs/2402.02834)等，给出了仅做Forward计算做深度裁剪，及其在内存受限场景上的有效性，裁剪后的CPT训练做能力恢复都是一个必要环节。

# Week-of-year: 12(2024-03-18 ~ 2024-03-24)
- LLM模型解释 

大模型在scale的过程中出现两个相对矛盾的现象：一方面实验观察到趋势清晰的Scaling Law，随着模型参数和训练数据的增大，评估集合上的平均损失（Loss）在幂指数级衰减，衰减系数可以相对准确的被拟合出来，呈现出稳定的可预测性；另一方面，模型训练过程中很多能力如上下文学习能力等会突然涌现出来，训练过程也存在突发的相移(Phase change)现象，与评估集的平均Loss的可预测下降形成鲜明对比。[J. Michaud](https://arxiv.org/abs/2303.13506)等给出了一个技能量化的解释，作者假设预测语言下一个token需要的技能由可数个小量化技能构成，每个量化技能是离散的，在模型训练过程中只有学到和没学到两个状态。量化技能在Loss下降贡献上存在差异，受量化技能可被用来预测下一个token的频度影响。频度服从Zipf幂分布的情况下，可以推导出Scaling Law。通过构造的稀疏奇偶校验问题，可以验证在模型参数、训练步数以及数据规模上的Scaling Law。进一步的分析表明大语言模型的Loss由多个小量化技能的复合构成，“单基因”token的Loss随模型参数增大出现突然下降，“多基因“token的Loss随模型参数变大呈现Scaling Law的规律。作者在Pythia-19M的模型上，演示了通过QDG方法定位的单个量化技能生成token的示例，cherry-pick的示例显示出一定的直观可解释性。

# Week-of-year: 11(2024-03-11 ~ 2024-03-17)
- LLM模型解释

围绕模型训练过程中出现的Grokking现象，在[A. Power](https://arxiv.org/abs/2201.02177)提出来后，多个团队在不同角度做了分析。针对取模加法的问题，[N. Nanda](https://arxiv.org/abs/2301.05217)用一层Transformer模型进行训练，并从Circuit功能与参数分析的角度给出了一个解释，发现训练的结果是实现了一个计算取模加法的Fourier算法，令人印象深刻。大模型的训练，本质是通过形成不同Circuit实现生成观察数据的算法的观点从单层模型上得到了一个验证。Deempind在[V. Varma](https://arxiv.org/abs/2309.02390)做了进一步的分析，说明Grokking现象是模型训练过程从记忆模型到泛化生成模型的一个转化，转化发生的阶段与训练数据规模有关，随着训练数据变大，记忆模型的效率变低，泛化模型的效率变高，转化发生也需要更多的epochs。训练开始阶段，实现的Circuit更多是记忆训练数据，模型参数权重绝对值也较大，随着训练的继续进行，训练损失下降到0，进一步的训练模型Circuit会逐步转化到泛化生成模型，模型参数权重绝对值降低，在测试集合上准确率开始提升，实现准确的泛化推广。先出现记忆再出现泛化的原因作者分析和不同Circuit的效率有关：1）如果记忆更容易降低损失，模型在训练过程中先学习到记忆Circuit，样本规模越小记忆越高效；2）训练损失为0后进一步学习可以得到泛化Circuit，与训练设定的Weight Decay有关，Weight Decay损失推动模型学习到低参数绝对值的泛化Circuit。

- LLM模型评估

通过大语言模型做大语言模型生成结果的评估，是模型评估的一个重要方向，具有低成本高效率的优势，不过也存在几个显著的问题：1）对比评估两个答案，对答案在Prompt中出现的位置比较敏感，倾向给首先出现的答案打高分；2）倾向于给内容更长的答案打高分；3）倾向于给自己生成的答案打高分；4）评估带有地域、政策、文化等偏差。在[L. Zheng](https://arxiv.org/abs/2306.05685)中，作者对前三个问题进行了分析并给出了解决方案，通过构造MT-Bench和Chatbot Arena两个数据集说明了大语言模型（GPT-4）做生成结果评估的可行性，评估结果与人工评估一致性超过80%。用GPT-4这种闭源大语言模型进行评估，也存在评估Prompt适配版本不稳定、大规模评估成本依旧偏高以及闭源带来的评估结果公正性的问题，在[S. Kim](https://arxiv.org/abs/2310.08491)中作者给了一个萃取GPT-4来微调开源大语言模型(LlaMA-13B)来训练具有稳定评估能力的模型。作者从50个种子评分规则出发，调用GPT-4构造1k评分规则，依据规则生成20k 指令，每个评分规则20条指令；针对20k的每条指令数据，生成5个答案及对应的评分反馈。用100k构造的样本数据，作者微调大语言模型并与人工标注及GPT-4标注进行对比，验证微调模型与人工评估及GPT-4评估的一致性。