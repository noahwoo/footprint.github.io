---
title: Weekly updates on token training of human intelligence
---
# Week-of-year: 12(2024-03-18 ~ 2024-03-24)
- LLM模型解释 

大模型在scale的过程中出现两个相对矛盾的现象：一方面实验观察到趋势清晰的Scaling Law，随着模型参数和训练数据的增大，评估集合上的平均损失（Loss）在幂指数级衰减，衰减系数可以相对准确的被拟合出来，呈现出稳定的可预测性；另一方面，模型训练过程中很多能力如上下文学习能力等会突然涌现出来，训练过程也存在突发的相移(Phase change)现象，与评估集的平均Loss的可预测下降形成鲜明对比。[J. Michaud](https://arxiv.org/abs/2303.13506)等给出了一个技能量化的解释，作者假设预测语言下一个token需要的技能由可数个小量化技能构成，每个量化技能是离散的，在模型训练过程中只有学到和没学到两个状态。量化技能在Loss下降贡献上存在差异，受量化技能可被用来预测下一个token的频度影响。频度服从Zipf幂分布的情况下，可以推导出Scaling Law。通过构造的稀疏奇偶校验问题，可以验证在模型参数、训练步数以及数据规模上的Scaling Law。进一步的分析表明大语言模型的Loss由多个小量化技能的复合构成，“单基因”token的Loss随模型参数增大出现突然下降，“多基因“token的Loss随模型参数变大呈现Scaling Law的规律。作者在Pythia-19M的模型上，演示了通过QDG方法定位的单个量化技能生成token的示例，cherry-pick的示例显示出一定的直观可解释性。

# Week-of-year: 11(2024-03-11 ~ 2024-03-17)
- LLM模型解释

围绕模型训练过程中出现的Grokking现象，在[A. Power](https://arxiv.org/abs/2201.02177)提出来后，多个团队在不同角度做了分析。针对取模加法的问题，[N. Nanda](https://arxiv.org/abs/2301.05217)用一层Transformer模型进行训练，并从Circuit功能与参数分析的角度给出了一个解释，发现训练的结果是实现了一个计算取模加法的Fourier算法，令人印象深刻。大模型的训练，本质是通过形成不同Circuit实现生成观察数据的算法的观点从单层模型上得到了一个验证。Deempind在[V. Varma](https://arxiv.org/abs/2309.02390)做了进一步的分析，说明Grokking现象是模型训练过程从记忆模型到泛化生成模型的一个转化，转化发生的阶段与训练数据规模有关，随着训练数据变大，记忆模型的效率变低，泛化模型的效率变高，转化发生也需要更多的epochs。训练开始阶段，实现的Circuit更多是记忆训练数据，模型参数权重绝对值也较大，随着训练的继续进行，训练损失下降到0，进一步的训练模型Circuit会逐步转化到泛化生成模型，模型参数权重绝对值降低，在测试集合上准确率开始提升，实现准确的泛化推广。先出现记忆再出现泛化的原因作者分析和不同Circuit的效率有关：1）如果记忆更容易降低损失，模型在训练过程中先学习到记忆Circuit，样本规模越小记忆越高效；2）训练损失为0后进一步学习可以得到泛化Circuit，与训练设定的Weight Decay有关，Weight Decay损失推动模型学习到低参数绝对值的泛化Circuit。

- LLM模型评估

通过大语言模型做大语言模型生成结果的评估，是模型评估的一个重要方向，具有低成本高效率的优势，不过也存在几个显著的问题：1）对比评估两个答案，对答案在Prompt中出现的位置比较敏感，倾向给首先出现的答案打高分；2）倾向于给内容更长的答案打高分；3）倾向于给自己生成的答案打高分；4）评估带有地域、政策、文化等偏差。在[L. Zheng](https://arxiv.org/abs/2306.05685)中，作者对前三个问题进行了分析并给出了解决方案，通过构造MT-Bench和Chatbot Arena两个数据集说明了大语言模型（GPT-4）做生成结果评估的可行性，评估结果与人工评估一致性超过80%。用GPT-4这种闭源大语言模型进行评估，也存在评估Prompt适配版本不稳定、大规模评估成本依旧偏高以及闭源带来的评估结果公正性的问题，在[S. Kim](https://arxiv.org/abs/2310.08491)中作者给了一个萃取GPT-4来微调开源大语言模型(LlaMA-13B)来训练具有稳定评估能力的模型。作者从50个种子评分规则出发，调用GPT-4构造1k评分规则，依据规则生成20k 指令，每个评分规则20条指令；针对20k的每条指令数据，生成5个答案及对应的评分反馈。用100k构造的样本数据，作者微调大语言模型并与人工标注及GPT-4标注进行对比，验证微调模型与人工评估及GPT-4评估的一致性。