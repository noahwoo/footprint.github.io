- **AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback, 2023, Stanford**
  - In one word: simulate human feedback with prompted LLMs to reduce the cost of LPF(learning from pairwise feedback), show the consistency with human feedback on performance ranking of 11 LPF algorithms
  - Method:
    - Instruction following dataset: 52K alpaca dataset
      - SFT: 10K
      - Pairwise preference(PREF): 10K
      - Unlabeled split: 20K used in algorithms such as PPO
      - Validation split: 2K for development and tuning
    - 3 key components for instruction following model efficiency
      - low-cost pair-wise feedback generator
        - Prompted GPT-4 model: 
          - guideline for appropriate response
          - in-context examples
          - batch generation
        - 13 simulated annotators for human variability
          - inter: different prompts
          - intra: flipping preference with 25% chance
      - automatic evaluation for methods development
        - win-rate from simulated evaluator
        - davinci-003 as reference model
        - evaluation dataset:
          - 805 instructions from OASST, HHA and Vicuna evaluation, etc.
      - reference implementation for comparison
    - 6 Algorithms in AlpacaFarm
      - Binary FeedME
      - Binary reward conditioning
      - Best-of-n sampling
      - Expert Iteration
      - PPO
      - Quark
    - Conclusions
      - Simulated annotators rank Algorithms highly correlated to human feedback: 0.98 spearman(too high)
      - Simulated annotators match human agreement: 65% aggrement rate with human majority vote
      - Simulated annotator replicate overoptimzation: indication of too much optimization on proxy criteria from reward model
      - Benchmarks on algorithm
        - SFT highly effective
        - PPO top the leaderboard
        - Best-of-n simple and effective
- **Can Large Language Models Be an Alternative to Human Evaluation?, 2023, NTU**
  - In one word: explore the ability of LLMs as evaluator in place of human, with results from open ended story generation and adiversial attacks, score text rated by LLMs is consistent with human ones
  - Methods: 
    - prompt LLMs to perform the evaluation task
      - task instruction
      - the sample to be evaluated
      - the evalation question
    - Human use the same format of prompt as LLM
    - For open-ended story generation
      - WritingPrompts dataset for human written story
      - Finetune GPT-2 medium model with WritingPrompts dataset for LLM story generator
      - 4 evaluation angles
        - Grammaticality: ```How **grammatically correct** is the text of the story fragment?```
        - Cohesiveness: ```How well do **the sentences** in the story fragment **fit together**?```
        - Likability: ```How **enjoyable** do you find the story fragment?```
        - Relevance: ```Now read the PROMPT based on which the story fragment was written. **Prompt**: [PROMPT]. How **relevant** is the **story fragment** to the **prompt**?```
      - 4 LLMs tested as evaluators: *T0, text-curie-001, text-davinci-003, ChatGPT*
      - Results:
        - Expert human evaluators prefer humanwritten stories
        - *T0* and *text-curie-001* do not show clear preference toward human-written stories
        - *text-davinci-003* shows clear preference toward human-written stories just like English teachers
        - *ChatGPT* rates like human experts and can explain its own decision well
        - Human experts mostly agree with the ratings and explanations of *ChatGPT*
        - *text-davinci-003* tends to give higher ratings and ChatGPT is the opposite
        - Weak to strong positive correlations between teachers’ ratings and *text-davinci-003*’s ratings
        - Evaluation and comparison are meaningful if using the same instruction
        - Changing the instructions and temperatures slightly change the absolute value of rating but does not chage the preference
- **KoLA: Carefully Benchmarking World Knowledge of Large Language Models, 2023, Tsinghua**