
# Transformers

- **Thinking Like Transformers, 2021**
- **Training compute-optimal large language models, 2022, Deepmind**
- **Formal Algorithms for Transformer, 2022, Deepmind**
    - In one word: a self-contained, mathematically precise overview of transformer architectures in pesudo-code
    - Notes: 
        - pesudo-code for each modules of Transformer: 
        - Token embedding
        - Position embedding
        - Attention
            - Single query attention
            - Attention for sequence
            - Multi-head attention
        - Layer-norm
        - FFN(absent due to simplicity)
        - Unembedding(decoding, or head in transformers lib. of huggingface)
        - pesudo-code for several transformer like architectures
        - Encoder-Decoder Transformer
        - Encoder Transformer
        - Decoder Transformer
        - misc
        - depict in matrix-vector product, instead of the mostly used vector-matrix product form
        - use bias $b$ in QKV projection: $k = W_k x+b_k$
- **[A precise and nano implementation of GPT2](https://github.com/karpathy/nanoGPT)**
- **Transformers: State-of-the-Art Natural Language Processing, 2020, Huggingface**
    - Targets
        - Extensible for researcher
        - Simple for practioner
        - Fast & robust for industrial deployment
    - Core modules
        - Transformers: MLM/Autoregression/Seq2Seq/Multimodal/Long-Distance/Efficient/Multilingual
        - Tokenizers: Char. Level BPE/Byte Level BPE/WordPiece/SentencePiece/Unigram/Char./Custom
        - Heads (Domain specific) : LM/Seq. Classification/QA/Token Classification(NER)/Multiple Choice/MLM/Cond. Generation
    - Deployment
        - model easy to switch from framework(say PyTorch) to the other one(say Tensorflow)
        - export to intermediate neural network format
        - use adapters to convert models to CoreML weights for edge devices
- **[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)**
- **[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)**
- **Efficient Transformers: A Survey, 2020, Google**
    - In one word: characterizes a large and thoughtful selection of recent efficiency-favored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains
- **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, 2019, Google**
    - In one word: a transformer architecture that enables learning dependency beyond a fixed length without disrupting temporal coherence
    - Method in details:
        - a segment level recurrence mechanism 
        - hidden state sequence of previous segment fixed and cached to be reused as extra (k,v) context in next segment
        - cached state sequence as a meory augumented neural networks $\bold{m} \in R^{M \times d}$
            - training: $M$ equals to segment length
            - inference: increase $M$ by several times for larger context(benefit from the inductive bias from relative positional encoding)
        - a novel relative Positional Encoding(PE) scheme
        - Absolute PE attention matrix $A_{i,j}$: $$(E_i+P_i)W_{Q}W_{K}^T(E_j^T+P_j^T)=E_i W_Q W_K^T E_j^T + E_i W_Q W_K^T P_j^T + P_i W_Q W_K^T E_j^T + P_i W_Q W_K^T P_j^T$$
        - For proposed relative PE, make three changes
            - $P_j$ -> $R_{i-j}$ for relative position from $i$ to $j$, $i \ge j$ in causal attention
            - positional constant $P_i W_Q$ change to $\bold{u}$ and $\bold{v}$, each for one of the two occurrences
            - split $W_K$ to $W_{K,E}$ and $W_{K,R}$ to use different key weight matrices for content and position
            - resulting relative PE formulation: $$E_i W_Q W_K^T E_j^T + E_i W_Q R_{i-j}^T + \bold{u} W_K^T E_j^T + \bold{v} W_K^T R_{i-j}^T$$
    - Main results
        - WikiText-103: new ppl(perplexity) SOTA from 20.5 to 18.3
        - enwik8: 12 layers reach new bpc(byte per character) SOTA 0.99, used only 17% parameters compared to a 64 layer network
        - text8: new bpc SOTA 1.08 by large margin
        - One Billon Word(short term dependency): new single-model SOTA of ppl 21.8 by large margin
        - Word level Penn Treebank: new SOTA without two-step finetuing
- **Unified Language Model Pre-training for Natural Language Understanding and Generation, 2019, MSRA**
    - In one word: a parameter-shared transformer framework for unified training of NLG and NLU tasks with attention masking
    - Method in details: 
        - Transformer Decoder: self-attention support bi-direction, uni-direction(causal) masking
        - 4 pre-training objectives
        - Unidirectional LM: ```x1x2x3[mask]x4```, ```[mask]``` attend to ```x1x2x3[mask]```
        - Bidirectional LM: ```x1x2x3[mask]x4```, ```[mask]``` attend to ```x1x2x3[mask]x4```
        - Seq2seq LM: ```[SOS]x1x2x3[EOS]x4x5x6[EOS]```
            - ```x2``` attend to ```[SOS]x1x2x3[EOS]```
            - ```x5``` attend to ```[SOS]x1x2x3[EOS]x4x5```
        - Next Sentence Prediction
        - 2 finetuning tasks
        - NLU: text classification
            - ```[SOS]``` embedding as sentence presentation
            - classification head and model parameters updated 
        - NLG: summarization
            - ```[SOS]S1[EOS]S2[EOS]```, random mask token at ```S2[EOS]``` 
            - masking of ```[EOS]``` enables model to terminate generation
- **White-Box Transformers via Sparse Rate Reduction**
- **Improving Transformer Optimization Through Better Initialization, 2020, layer6ai**
  - In one word: 
- **RoFormer: Enhanced Transformer with Rotary Position Embedding, 2021, Zhuiyi**
  - In one word: encode position by a rotation for every 2-dim of the input vector, attention calculation extends to the complex number dot-product, and take the Real part
  - Method:
    - Position Encoding by rotating each pair of dimension: $f(x, m) = [(x_0 + i x_1) e^{i m \theta_1}, (x_2 + i x_3) e^{i m \theta_2}, \cdots, (x_{d-2} + i x_{d-1}) e^{i m \theta_{d/2-1}}]$
    - Attention with position encoding: $Re<f(q, m), f(k, n)> = a(m-n)$
    - Applied for Q, K in each layer
    - Long-term decay(but not tight):
      - $a(m-n)$ is bounded by $(\max |h_{j+1} - h_j|) \sum_{j=0}^{d/2-1} |S_{j+1}|$
      - where $h_j = q_{[2j, 2j+1]} k_{[2j, 2j+1]}^*$, $S_j = \sum_{t=0}^j e^{i (m-n) \theta_t }$
- **Extending Context Window of Large Language Models via Positional Interpolation, 2023, MetaAI**
  - In one word: extend the context window size of pretrained RoPE-based LLM by position interpolation the short window, by linearly down-scale the input position indices to match the original context window size
  - Method: 
    - Scaling down the input position $m$ to pretrained context window: $\hat{f}(x,m') = f(x, m' \frac{L}{L'}$
    - where $L' > L$ is the new context window size
    - $\hat{a}(m'-n') =: Re<\hat{f}(q, m'), \hat{f}(k, n')>$
    - Denote $s= (m'-n') \frac{L}{L'}$, then $s \in [s_1, s_2]$, and $s_2 - s_1 = 1$
    - Bound: $|\hat{a}(s) - \hat{a}_{linear}(s; [s_1, s_2])| \leq \frac{d (\max_j |h_j|)}{32 \log 10000}$
- **Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2021, UW**
  - In one word: verify the expolation of position encoding using perplexity on sequence longer than training, proposed a simple linear bias to the attention matrix, which shows good expolation performance
  - Method: 
    - Change attention matrix from $\text{softmax}(q_i K^T)$ to $\text{softmax}(q_i K^T + m \cdot [-(i-1), \cdots, -2, -1, 0])$
    - Slope $m=2^{-\frac{8i}{n}}$ for each $i$ of $n$ head
    - Implement by modify the self-attention mask matrix $T \times T$, change to $n \times T \times T$
  - Experimental conclusions
    - Sinusoidal extrapolation declines after 50 tokens for L=1024
    - Rotary extrapolates to 100 for L=1024
    - T5-bias extrapolates to 800 for L=1024
    - ALiBi exptrapolates to any length sequence!