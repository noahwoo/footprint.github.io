---
title: Large Language Models on The Way
author: Jianmin
date: 3/2/2023
---

# 核心技术方向
- Pretrained Models
   - Left to right(GLM)
      - OpenAI: GPT/GPT-2/GPT-3
   - Masked LM
      - Google: BERT
      - Google: RoBERTa
   - Prefix LM
      - MSRA: UniLM1/UniLM2
   - Encoder/decoder
      - Google: T5
      - MSRA: MASS
      - Facebook: BART
- Prompt Engineering
   - Shape: 
      - Cloze: LAMA/TemplateNER
      - Prefix: Prefix-Tuning/Prompt-Tuning
   - Human Effort:
      - Hand crafted: LAMA/GPT3
      - Automated: 
         - Discrete: AdvTrigger/AutoPrompt
         - Continuous: Prefix-Tuning/Prompt-Tuning
- Answer Engineering

# Resources on track
- Transformers
   - **Thinking Like Transformers, 2021**
   - **Training compute-optimal large language models, 2022, Deepmind**
   - **Formal Algorithms for Transformer, 2022, Deepmind**
      - In one word: a self-contained, mathematically precise overview of transformer architectures in pesudo-code
      - Notes: 
         - pesudo-code for each modules of Transformer: 
            - Token embedding
            - Position embedding
            - Attention
               - Single query attention
               - Attention for sequence
               - Multi-head attention
            - Layer-norm
            - FFN(absent due to simplicity)
            - Unembedding(decoding, or head in transformers lib. of huggingface)
         - pesudo-code for several transformer like architectures
            - Encoder-Decoder Transformer
            - Encoder Transformer
            - Decoder Transformer
         - misc
            - depict in matrix-vector product, instead of the mostly used vector-matrix product form
            - use bias $b$ in QKV projection: $k = Wx+b$ 
   - **[A precise and nano implementation of GPT2](https://github.com/karpathy/nanoGPT)**
   - **Transformers: State-of-the-Art Natural Language Processing, 2020, Huggingface**
      - Targets
         - Extensible for researcher
         - Simple for practioner
         - Fast & robust for industrial deployment
      - Core modules
         - Transformers: MLM/Autoregression/Seq2Seq/Multimodal/Long-Distance/Efficient/Multilingual
         - Tokenizers: Char. Level BPE/Byte Level BPE/WordPiece/SentencePiece/Unigram/Char./Custom
         - Heads (Domain specific) : LM/Seq. Classification/QA/Token Classification(NER)/Multiple Choice/MLM/Cond. Generation
      - Deployment
         - model easy to switch from framework(say PyTorch) to the other one(say Tensorflow)
         - export to intermediate neural network format
         - use adapters to convert models to CoreML weights for edge devices
   - **[A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)**
   - **[In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)**

- OpenAI GPT series: 
   - GPT-1: **Improving Language Understanding by Generative Pre-Training, 2018, OpenAI**
   - GPT-2: **Language Models are Unsupervised Multitask Learners, 2019, OpenAI**
   - GPT-3: **Language Models are Few-Shot Learners, 2020, OpenAI**
   - **InstructGPT: Training language models to follow instructions with human feedback, 2022, OpenAI**
      - Reference:
         - **Deep Reinforcement Learning from Human Preferences, 2020, Deepmind/OpenAI**
         - **Fine-Tuning Language Models from Human Preferences, 2020, OpenAI**
         - **Learning to summarize from human feedback, 2021, OpenAI**
         - **Instruct-tuning: Finetuned language models are zero-shot learners, 2022, Google**
   - **Fine-Tuning Language Models from Human Preferences, 2020, OpenAI**
      - Tasks: Stylistic continuation(in sentiment or genre) and Summarization
      - Optimize over $(x, {y_i}, b)$, LogLikelihood
      - Logits consist of reward from LM $r(x,y)$ and KL dist. between new and old model
      - experiments:
         - summarization: pretrain 774M GPT-2 LM, then RL fine-tune
         - stylistic continuation: train with WebText from scrath, SFT on BookCorpus, then RL fine-tune
   - **Learning to summarize from human feedback, 2021, OpenAI**
      - Goal: advance methods for training language models on objectives that more closely capture the behavior we care about. 
      - Task: Text summarization on Reddit TL;DR dataset
      - Annotation quality: aggreement level between labeler and researcher
         - charge by working hours instead of # of annotated samples.
         - hands-on with annotators closely
         - a detailed procedure for anotation
      - Four steps in loop
         - pre-trained model: 
            - Corpus: C4/Webtext/books/Wikipedia
            - 1~3 epoch on each, 300B tokens, context 2048
         - supervised model
            - predict summary from Reddit summaries
            - initialized from pre-trained model, cosine LR schedule, log lineear sweep
            - batch size 128
            - sampling with T=0 for pair-wise evaluaiton by human
            - supervised model outperform SOTA on ROUGE score of CNN/DM dataset
         - reward model
            - initialized from supervised model, replace the decoding matrix with a linear head to output scalar value
            - use pair-wise loss of preferred summary to unpreferred one
         - Reinforcement Learning from Human Feedback
            - use separated transformer parameters for policy and value networks
            - BPE token generation as each step of episode, final reward model score as part of the RL Reward
            - KL distance between policy and init. supervised model also included in final Reward
            - policy network initialized with supervised model
            - value network initialized with reward model
      - Results & Conclusions   
   - Intelligence in GPT series
      - GPT-3: (davinci)
         - generation ability
         - world knowledge
         - in-context learning
      - Instruct tuning (davinci-instruct-beta)
         - instruction follow and unseen task generalization
      - Add code training (code-davinci-002)
         - code understanding
         - complex reasoning/CoT(possibly)
         - best of all: text doc and code combined
      - Supervised tuning (text-davinci-002) + RLFH (text-davinci-003)
         - ability to alignment with human
         - reject unknown/illegal/unethical question
   - ChatGPT Plugins
      - [An end-to-end 3'rd demo](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/how-chatgpt-plugins-could-work/ba-p/3761483)

- Google Research/Brain/DeepMind: 
   - BERT series: 
      - **BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Google** (OK)
      - **RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019, Facebook** (OK)
   - LaMDA series: (Language model for Dialog Application)
      - **Towards a Human-like Open-Domain Chatbot, 2020, Google**
      - **Finetuned language models are zero-shot learners, 2022, Google** (LaMDA-PT -> FLAN) (OK)
         - instruct tuning for multi-task generalization, with 60+ NLP tasks
      - **Scaling Instruction-Finetuned Language Models, 2022, Google** (Flan series: T5/PaLM) (OK)
         - scaling instruction tasks to 1.8k 
         - scaling model size from 80M to 540B
         - fine-tuning on CoT data
      - **The Flan Collection: Designing Data and Methods for Effective Instruction Tuning, 2023, Google**
   - GLaM series: (More Efficient In-Context Learning, sparse language model(FFN -> MoE))
      - **Efficient Scaling of Language Models with Mixture-of-Experts, 2022, Google**
   - PaLM series: (540-Billion parameters, attention and ffn computed in parallel)
      - **PaLM: Scaling Language Modeling with Pathways, 2022, Google, Jeff Dean**
   - Chinchilla: (optimal model size and #tokens for training a transformer language model under a given compute budget)
      - **Training Compute-Optimal Large Language Models, 2022, (Google/DeepMind)**
   - Gopher: (an analysis of Transformer-based language model performance across a wide range of model scale)
      - **Scaling Language Models: Methods, Analysis & Insights from Training Gopher, 2022, (Google/DeepMind)**
   - T5 series: (training all kinds of task in unified text-to-text way)
      - **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2020, Google (T5)**

- More opensource model:
   - **BLOOM: A 176B-Parameter Open-Access Multilingual Language Model, 2022, MetaAI**
      - In one word: towards democratizing LLM with BLOOM, a 176B-parameter open access decoder only language model
      - BLOOM traininng details
         - Datasets
         - Model architecture
         - Tokenization
         - Engineering
         - Training
      - Evaluation
         - SuperGLUE
         - Machine Translation
         - Summarization
         - Code generation
         - Text embedding representation
         - Multilingual probing
   - **LLaMA: Open and Efficient Foundation Language Models, 2022, MetaAI**
      - In one word: possible to train LLM with publicly avaliable dataset exclusively, with inference budget in consideration
      - Approach: 
         - 7 pretraining datasets: publicly available
            - English CommonCrawl(67%): pre-process with CCNet pipeline
               - dedup at sentence level
               - remove non-English pages
               - filter low-quality content with n-gram
               - discard pages not classified as Wiki references?!
            - C4(15%): diverse pre-process CommonCrawl improve performance, pre-process the same as CCNet, except
               - filter low-quality content by heuristics
            - Github(4.5%): public Github data from Google BigQuery
               - filter low quality files by line length or proportion of alphanumeric chars
               - remove boilerplate
               - dedup at file level, exact match
            - Wikipedia(4.5%): June-August 2022 dump
               - remove hyperlinks, comments, and other formatting boilerplates
            - Gutenberg and Books(4.5%):
               - dedup as book level, remove books with 90% content overlap
            - ArXiv(2.5%)
               - start from first section, no bibliography
               - remove comments and inline definition and macros
            - Stack Exchange(2%)
               - 28 largest websites kept
               - remove HTML tags
               - sort answers by score
         - Tokenizer: BPE encoding from SentencePiece, 1.4T tokens overall, 2 epoches for Wikipedia and Book tokens
         - Architecture: 
            - Encoder only
            - Pre-normalization: normalize the input of each sublayer of transformer
            - SwiGLU activation: 4*2/3*d hidden dim. in FFN(PaLM)
            - Rotary positional embedding(GPTNeo)
         - Optimizer: 
            - AdamW with $\beta_1$=0.9, $\beta_2$=0.95
            - consine learning rate schedule
            - weight decay 0.1
            - gradient clip 1.0
            - 2000 warmup steps, LR varies with batch size
         - Efficient implementation
            - not storing attention weights
            - not computing masked key/query scores
            - same expensive activations such as output of linear layers, by manually implement the backward function
            - 380 tokens/sec/GPU on 2048 A100 with 80GB RAM, 1.4T token requires 21 days
         - Evaluation tasks: 
            - Common sense reasoning: BoolQ/PIQA/SIQA/HellaSwag/WinnoGrande/ARC easy and challenge/OpenBookQA
            - Closed-book QA: Natural Question/TriviaQA
            - Math. reasoning: MATH/GSM8K
            - Code generation: HumanEval/MBPP
            - MMLU: perform low, limited amount of books used for pre-training
         - Bias, Toxicity and Misinformation
            - RealToxicityPrompts: toxicity increase with increase of model size
            - CrowS-Pairs: bias evaluation
            - WinoGender: gender bias
            - TruthfulQA: truthful and informative
   - **GLM: General Language Model Pretraining with Autoregressive Blank Infilling, 2022, Tsinghua**
      - In one word: a general language model based on autoregressive blank infilling objective for the NLU, generation and conditional generation tasks in one model
      - Methods: 
         - span infilling as in T5
         - random span order in autoregression
         - 2D position defined for span tokens to generate span with no predefined span length
         - Multi-task pretraining: longer span infilling
            - Document Level: 50% ~ 100% length of original text as span
            - Sentence Level: whole sentence as span, sample 15% tokens 
         - Finetune with classification tasks
            - Sentiment classification: `{SENTENCE}. It is really [MASK].`

- Distributed training and inference
   - Data parallel
      - **[DDP PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)**: 
         - In one word: all-reduce after back-propagation
      - **ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, 2019**
         - shards optimizer states, gradients, parameters to data parallel workers
         - each worker updates the local parameters by forward/backward
         - global parameter and gradient synced by all-gather and reduce-scatter in each layer, RAM released for next layer
      - **[FSDP: PyTorch offical version of ZeRO](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)**
         - In one word: replacement for DDP, pesudo-code in nutshell:
            ```
            forward pass :
            for layer_i in layers:
               all-gather full weights for layer_i
               forward pass for layer_i
               discard full weights for layer_i
            backward pass:
            for layer_i in layers:
               all-gather full weights for layer_i
               backward pass for layer_i
               discard full weights for layer_i
               reduce-scatter gradients for layer_i
            ```
   - Model parallel
      - Tensor Parallel: horizontal partition
         - **[Mesh TensorFlow - Model Parallelism Made Easier](https://github.com/tensorflow/mesh)** 
            - In one word: for TensorFlow
         - **Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism, 2019**
            - In one word: for PyTorch
      - Pipeline Parallel: vertical partition
         - **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism, 2018**
         - **[PipeDream](https://github.com/PipedreamHQ/pipedream)** 
            - In one word: improve pipeline efficiency by storing multiple copies of weights
         - **TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models, 2021**: 
            - In one word: designed for Transformer, pipeline occurs across tokens instead of mini-batch
   - All in one
      - **Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel Training, 2021**
      - **[Megatron-Deepspeed](https://github.com/microsoft/Megatron-DeepSpeed)** 

- Instruct Finetuning
   - **SELF-INSTRUCT: Aligning Language Model with Self Generated Instructions, 2022**
      - In one word: improve the instruction following capabilities of pretrained LM by bootstrapping off its own generation
      - Methods: definition of instruction data -> $(I_t, [X_t, Y_t]+)$ , $X_t$ can be empty
         1. Instruction generation
            - 175 tasks pool with instruction from human annotation, more from bootstrapping in the following
            - sample 8 tasks from the pool, 6 from human annotation, 2 from model generated tasks
            - in-context learning prompt like: `Come up with a series of tasks: \n\n Task 1: {Instruction for existing task-1} \nTask 2: {Instruction for existing task-2}\n...\nTask 8: {Instruction for existing task-8\nTask 9: }`
         2. Identify if the instruction represents a classification task
            - task type(classification or not) used for instance generation
            - 12 classification and 19 non-classification few-shot prompts
         3. Instance generation: few-shot prompting with instruction-input-output demonstations
            - Input-first approach: for non-classification task
               - LLM first come up the input
               - then produce the output based on the input
            - Output-first approach: for classification task, solve the label inbalance problem
               - generate the output label first
               - generate the input condition on the output label
         4. Filter low-quality data
               - instruction diversity: use instruction with ROUGE-L overlap score < 0.7 for any existing ones
               - instance diversity and consistency: dedup instance, filter out instance with same input but different output
         5. Finetune LM to follow the instructions
      - Results: 
         - 52K instructions and 82K instances generated with good diversity and quality(92% valid instruction, 54% valid input/output and instruction)

- RLFH series:
   - **Illustrating Reinforcement Learning from Human Feedback (RLHF), 2022, Hugging Face Blog**
      - [Github Link](https://github.com/huggingface/blog/blob/main/rlhf.md)
   - **Deep reinforcement learning from human preferences, 2017, OpenAI**
   - **A survey of preference-based reinforcement learning methods, 2017, JMLR**

- Prompt(hard/soft) engineering
   - Survey:
      - **Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing, 2021, CMU** (OK)
      - **[Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/), 2023, OpenAI**
   - Parameter efficient tuning
      - **Prefix Tuning: Optimizing Continuous Prompts for Generation, 2021, Stanford** (OK)
         - **[Code in Github](https://github.com/XiangLi1999/PrefixTuning)**
            - Autoregressive LM(GPT2) and encoder-decoder architecture(BART)
            - GPT2 in details: 
               - based on GPT2 huggingface implementation(oooold version)
               - hooked in GPT2 by the `past_key_values` arguments in `forward` method
               - `past_key_values` constructed in the following steps:
                  - `pre_seq_len` hidden tokens with `n_embd`-dim embedding vector
                  - embedding vector passed through an MLP with one hidden layer with default size 512, and output size `2 * n_layers * n_emb(=num_head * head_n_embd)`, here `2` for key and value vectors
                  - shape of `past_key_values`: `(2, batch_size, num_head, seq_len, head_n_embd)`
      - **Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning, 2021, Google** (OK)
      - **P-tuning/P-tuning V2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks, 2021, Tsinghua** (OK)
      - **Unified View: Towards a unified view of parameter-efficient transfer learning, 2022, CMU** (OK)
         - In one word: Unify Adpater/LoRA/Prefix-tuning into the modification to specific hidden states(heads) in pretrained model
         - Aspects of modifications
            - target: head-attention(Prefix Tuning), attention(Adapter/LoRA), ffn(Adapter), key/value tranform matrix(LoRA)
            - composition: $h \leftarrow h + s \Delta h$ or $h \leftarrow (1-\lambda(x)) h + \lambda(x) \Delta h$ (PrefixTuning)
            - modifier $\Delta h$: 
               - low rank bottleneck: $f(v W_1)W_2, W_1 \in R^{d \times l}, W_2 \in R^{l \times (d|d_h)}$
               - $v$: PLM layer input $x$(Prefix Tuning/LoRA) or (head-)attention $h$(Adapter) 
               - $f$: Identity mapping(LoRA), ReLU activation function(Adapter), Softmax function(Prefix Tuning)
               - parallel(PrefixTuning/LoRA) or sequential(Adapter)
               - scaling or not: yes(LoRA), no(Prefix Tuning/Adapter)
         - Results
            - Claimed comparable performance to fine-tuning not generalize well to other benchmark
            - parallel adapter beats sequential adapter
            - ffn modification utilize the added parameters more effectively than (head-)attention, except for the case with less than 0.1% parameters added
            - scaling composition function better than vanilla additive one
            - Mix-And-Match adapter utilizing the good of Prefix-tuning and Adapter works better
      - **Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning, 2020, Facebook** 
      - **Revisiting Parameter-Efficient Tuning: Are We Really There Yet?, 2022**
      - **Parameter-efficient fine-tuning of large-scalepre-trained language models, 2023, Tsinghua**
      
      - **[LoRA for Stable Diffusion Finetuning](https://huggingface.co/blog/lora)**
         - **[Github](https://github.com/cloneofsimo/lora)**
         - **DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, 2022, Google**
            - In one word: personalization of text-to-image diffusion models by finetuning
         - **An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion, 2022, NVIDIA**
         - **Pivotal Tuning for Latent-based Editing of Real Images, 2021**
         - **[The Illustrated Stable Diffusion](https://jalammar.github.io/illustrated-stable-diffusion/), 2022**
         - **[The Annotated Diffusion Model](https://huggingface.co/blog/annotated-diffusion), 2022**
   - Automation:
      - **Large Language Models Are Human-Level Prompt Engineers, 2023**
   - Applications: 
      - **Prompt tuning GPT-2 language model for parameter-efficient domain adaptation of ASR systems, 2022, Amazon** (OK)

- In context learning
   - Survey
      - **A Survey on In-context Learning, 2023, PKU** (OK)
   - Explanations:
      - **Rethinking the Role of Demonstrations: What Makes In-Context LearningWork?, 2022, Facebook**
      - **An Explanation of In-context Learning as Implicit Bayesian Inference, 2022, Stanford**
      - **[How does in-context learning work? A framework for understanding the differences from traditional supervised learning](https://ai.stanford.edu/blog/understanding-incontext/), 2022, Stanford**
         - In one word: 
            - In context learning as a Bayesian inference of the prompt concept that every example in the prompt shares: $z$ in $P(z|p)$
         - Methods
            - Pretraining distribution: assume LLM fits the pretraining distribution exactly
            - Prompt distribution: In context prompt examples are drawn from the same prompt concept
            - Bayesian inference for in context learning formally:
               - $P(o|p) = \int_{z} P(o|p,z) P(z|p) dz$, $p$ for prompts, $z$ for latent concept, $o$ output of LLM
               - $P(z|p)$ concentrates on the prompt concept with more examples in the prompt 
               - Noise and signal
                  - Training prompt examples privide signal: Strong enough to overwrite the noise
                  - Transition between examples in low prob.: unnatural text, different from the pretraining distribution
                  - In-context learning robust to noise
            - Empirical evidence
               - Forming the prompt with the ground truth output is not required to achieve good in-context learning performance
               - The underlying input distribution that examples are drawn matters
               - The set of outputs in the task and input-output format matter
               - One more evidence: In-context learning performance is highly correlated with term frequencies during pretraining
            - Avenues for extensions
               - Input-output ground truth mapping matters for synthetic tasks
               - Instruction as improving Bayesian inference by providing explicit observations of the latent prompt concept
               - Preference of pretraining data for eliciting in-context learning
      - **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, 2023, Stanford**
      - **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers, 2022, Tsinghua**

- CoT/Reasons: 
   - **Chain of thought prompting elicits reasoning in large language models, 2022, Google** (OK)
      - Prompt with chain of thought helps improve perf. on arithmetric, commonsense and symbolic reasoning
      - Ablation study shows: 
         - equation only prompting not work
         - expressing intermediate steps via natual language helps
         - sequential reasoning embodied in chain of thought is useful for reasons beyond just activating knowledge
   - **Self-consistency improves chain of thought reasoning in language models, 2023, Google**
   - **Reasoning with Language Model Prompting: A Survey, 2022, Alibaba**
      - In one word: survey of cutting-edge research on reasoning with language model prompting
      - Methods
         - Strategy Enhenced Reasoning: design better reasoning strategy, like CoT/Multi-stage CoT
            - Prompt Enginneering: 
               - Single stage: 
                  - CoT works; more Chain steps, better results
                  - Exemplars' quality, diversity and order matters
                  - Even zero-shot works by prompting with 'Let's think step by step'
               - Multi stage: decompose into subtasks, remedy compositionality gap
                  - Reasoning step by step in sequential, concate subtask question to the end of prompt
                  - Split and merge tasks with different prompts
            - Process optimization: Natural language rationales
               - Reasoning consistency and continuity matters
               - Self Optimization
               - Iterative Optimization
                  - StAR: a small set of exemplars to generate reasoning steps, ones lead to correct answer used for fine-tuning
                  - generates multiple reasoning processes, the most consistent ones used for fine-tuning
                     - Selfconsistency improves chain of thought reasoning in language models, 2022, Google
               - Ensemble Optimizization
                  - not all reasoning steps should undertake the incorrectness of answers
                  - sampling multiple reasoning processes, generate most consistant answer by vote
            - External engine: 
               - 2 goals in LLM: semantic understanding and complex reasoning, first one can leverage external system
               - examples of external engine(no world knowledge, just rules)
                  - Physical simulator: 
                  - Code interpreters: PAL/POT
         - Knowledge Enhenced Reasoning
            - Implict knowledge: elicit knowledge from LM by few-shot prompting, then prompot downstream LM
            - Explict knowledge: retrieval augumented LM
               - Learning to retrieve prompts for in-context learning, 2022
               - Rainier: Reinforced knowledge introspector for commonsense question answering, 2022
      - Tasks
         - Arithmetic/Commonsense/Symbolic/Logical/Multimodel reasoning
         - Visualcomet: Reasoning about the dynamic context of a still image, 2020 (GPT-4 image reasoning demo)
   - **Towards Reasoning in Large Language Models: A Survey, 2022, UIUC**
   - **ReAct: Synergizing reasoning and action in language models, 2022, Google** (OK)
   - **Show Your Work: Scratchpads for Intermediate Computation with Language Models, 2021, Google Brain**
      - In one word: fine-tune Transformers to perform multi-step computations(long digits addition/program exec.) with intermediate computation steps as prompt written into a “scratchpad”
      - Conclusions: 
         - encoding long digits addition process step by step (add&carry) as text and doing supervised fine-tuning improve the performance of long digits addition tasks
         - encoding polynomial evaluation process step by step (item by item) as text and fine-tuning improve the performance of polynomial evaluation tasks
         - emitting full program traces line by line annotated with local variables improve the performance of computer program execution prediction
      - Method & Experiments
         - Use decoder-only transformer models, pretrained model ranging in size from 2M to 137B parameters
         - Addition task
            - Train with 1-8 digits addition task ,test with 1-8 in-distribution and 9-10 out-of-distribution tasks
            - Fine-tuning with 100K examples for 5K steps, batch size 32
            - Test with 10K in-distribution and 1K out-of-distribution examples
            - Scratchpad fine-tuning models perform well with the scaling of model size
   - **StAR: Bootstrapping reasoning with reasoning, 2022, Google**
      - In one word: iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform more complex reasoning
      - Methods:
         - In brief:
            1. prompt with few-shot rationale examples, answer questions
            2. if answer wrong, generate rationale with correct answer (Rationalization)
            3. fine-tuning on &lt;P, rationale, C&gt; with rationale leading to or rationalized with correct answer from step 1 and step 2
            4. repeat from step 1 (hence the 'bootstrap')
         - In details: 
            - almost the same as in brief
            - a view in RL: $J(M,X,Y) = \sum_i \mathcal{E}_{\hat{r_i}, \hat{y_i} \sim P_M(.|x_i)} \mathcal{1}(y_i=\hat{y}_i)$
            - Rationalization implemented by mark the correct answer with '(CORRECT)' in prompt
         - Experiments:
            - Arithmetic: n-digits sum with scratchpad
            - CommonsenseQA: 12k question with five choices
            - GSM8K: grade-school-level word problem
   - **Self-ask: Measuring and narrowing the compositionality gap in language models, 2022, MetaAI**
      - In one word: measure and solve the problem of compositional gap: the fraction of incorrectly answered questions with correct sub-problems answers by model
      - Methods: CoT and self-ask works, self-ask perform better with the help of search engine
         - In breif: Similar to least-to-most, prompt with examples
            - started with 'Are follow up questions needed here: Yes/No'
            - followed with Intermediate Question and Answer from LM or Search Engine
            - repeat until all the follow-up question answered
            - 'So the final answer is:' followed up with final answer
         - Details:
            - measure the compositionality gap:
               - a model can compose facts at a much higher rate when it can recall these facts more confidently
            - CoT & Self-ask
               - CoT and self-ask let the model apply more computation to harder problems by demonstrating the reasoning chain
            - Employ search engine for realtime world knowledge
               - answer the follow-up questions by search engine(feature snippets or snippets from top ranked result)
      - Experiments & datasets
         - Compositional Celebrities dataset
         - 2WikiMultiHopQA
         - Bamboogle: a dataset of 125 questions
            - reading random Wikipedia articles and writing a 2-hop question about them
            - fitler the questions that can be answered by search engine correctly
   - **Least-to-most prompting enables complex reasoning in large language models, 2022, Google**
      - In one word: Solve problem harder than the demonstration examples in CoT prompt
      - Methods: least to most prompting, using a progressive sequence prompts to help language model learn a new skill
         - In brief: based on few-shot prompting
            - reduce the complex problem in many sub-problems by querying LLM
            - each sub-problem solved sequentially by querying LLM, with answers from the previous sub-problem as facilitating prompts
         - In details
            - Stage 1: reduce to subproblems: prompt LM with examples of how to reduce the complex problem into subproblems, follow by a specific problem to reduce
            - Stage 2: solve subproblems sequentially, with complex problem appended as the last one
               - prompt LM with examples of how the first subproblem solved, followed by the first subproblem
               - take the answer from LM, append it to the previous prompts, followed by the next subproblem
               - continue util reach the last subproblem, i.e., the complex problem, return the answer then
         - Experiments: Output zero-shot and CoT in all datasets
            - Symbolic manipulations: last-letter-concatenation
            - Compositional generalization: SCAN
            - Math reasoning: DROP & GSM8K
   - **Multimodal chain-ofthought reasoning in language models, 2023, Amazon(Alex Smola)**
   - Automatic CoT
      - **Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data, 2023**

- World knowledge and augumented language model
   - **REALM: Retrieval-Augmented Language Model Pre-Training, 2020, Google** (OK)
      - Neural retrieval augumented LM generator: P(y|x) = \sum_z P(z|x) P(y|x,z)
      - P(z|x) a transformer encoded retrieval with vector similarity as the ranking criteria
      - Marginalize on z over Top-K approximation to reduce the computation burden
   - **In-Context Retrieval-Augmented Language Models, 2020, AI21 Labs** (OK)
   - **RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, 2021, Facebook** (OK)
      - Similar idea as REALM
      - Seq2seq language model generator: BART
      - Neural retrieval: dense passage retrieval(DPR) for Wikipedia documents, with two transformer encoders for query and doc
      - Document encoder fixed to pretrained bi-encoder DPR model on TriviaQA and Natural Questions
      - Learned retrieval ablation tested, show performance gain
      - Hotspot index swap for knowledge update works
   - **MRKL(miracle): Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning, 2022, AI21** (OK)
      - Call for neural and symbolic combined approach, to overcome the limitation of neural language model
      - Arithmetic as a test case, focus on the neural router to extract operands and operators from arithmetical question
      - Templates are used to generate <prompt, completion> pairs for fine tuning of neural router
      - Prompt tuning employed for soft-prompt in experiments
   - **RETRO: Improving language models by retrieving from trillions of tokens, 2022, Google/DeepMind**
   - **WebGPT: Browser-assisted question-answering with human feedback, 2022, OpenAI**
      - In one word
         - fine-tuned on question-human demonstration pairs
         - optimization via RLHF
      - Method on details, four aspects
         1. behavior cloneing(SFT/Bahavior Cloning)
         2. reward modeling(RM)
         3. reinforcement learning(RLFH)
         4. reject sampling (an alternative to #3, in exchange of offline training with online inference)
      - Results
         - ELI5 dataset: 56% preferred over people, 69% preferred over top-voted by ELI5 dataset
         - TruthfulQA dataset: truth 75% of the time, truth and informative 54% of the time
      - Ref. 
         - Learning to summarize from human feedback, 2020, OpenAI
   - **DSP: Composing retrieval and language models for knowledge-intensive NLP, 2023, Stanford**
   - **Knowledge Retrieval Architecture for LLM’s** (OK)
      - [Web Link](https://mattboegner.com/knowledge-retrieval-architecture-for-llms/)
   - **Augmented Language Models: a Survey, 2023, Facebook** (OK)
      - Key elements (Langchain abstract as Agent & Tools)
         - Reasoning: given more computation steps to the model before yielding the answer to a prompt
         - Tool: getting each step right
         - Action: call tool and observe the result
      - Augument LM's reasoning capability
      - Allow LM to interact with exteral tools and act
      - Reasoning and tools usage implemented by
         - Supervision
            - Few shot prompting
            - Gradident-based learning(instruct tuning mostly used)
               - Blender Bot
               - WebGPT
            - Prompt pre-training: mix pre-training data with labeled demo. of reasons
               - Galactica
            - Bootstrapping: prompt LM to reason or act in few shot setup with final prediction, examples lead to incorrect prediction removed, initial LM fine-tuned on correct examples 
               - **STaR: Self-taught reasoner bootstrapping reasoning with reasoning, 2022**
               - **Talm: Tool augmented language models, 2022**
         - Reinforcement Learning
            - Human preference data(ranking/like/dislike): from SFT to RL
            - Most RL work teach LM to act rather than reason(besides STaR)
            - Hardcode reward:
               - **Conqrr: Conversational query rewriting for retrieval with reinforcement learning, 2022**
               - **Rainier: Reinforced knowledge introspector for commonsense question answering, 2022**
               - **Webshop: Towards scalable real-world web interaction with grounded language agents, 2022**
               - **Regen: Reinforcement learning for text and knowledge base generation using pretrained language models, 2021**
            - Human feedback(RLFH): Alignment
               - **TAMER: Training an agent manually via evaluative reinforcement, 2008**
               - **Deep TAMER: Interactive agent shaping in high-dimensional state spaces, 2018**
               - **Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization, 2022**
               - InstructGPT
               - **WebGPT**
               - **Internet-augmented language models through few-shot prompting for open-domain question answering, 2022, Deepmind**
               - **GopherCite: Teaching language models to support answers with verified quotes, 2022**
               - **Toolformer: Language models can teach themselves to use tools, 2023**
      - Discussion
         - **Looped transformers as programmable computers, 2023**
         - **A path towards autonomous machine intelligence, 2022, Lecun**
         - **Language models (mostly) know what they know, 2022**
         - **React: Synergizing reasoning and acting in language models, 2022**
   - **Talm: Tool augmented language models, 2022**
      - In one word: text-only approach to augument LM with non-differential tools, self-play technique to bootstrap performance with few tool usage demonstrations at the beginning
      - Methods:
         - Task and tool interface: *task input text |tool-call too input text |result tool output text |output task output text*
         - Self-play: 
            1. start with bootstrap set $D = \{(x_j, t_j, r_j, y_j)\}$
            2. finetune LM with $D$
            3. for each examples in task, sample tool & input($t_j$), call tool to get the result, sample task output
               - use high temperature(1.0) and sample size (topk=40) to explore text of diverse tool API invocation 
            4. if task output match target $y_j$ within threshold, add $(x_j, t_j, r_j, y_j)$ to D
            5. goto step 2
         - RL analogy:
            - policy-gradient with LM as policy $\pi(t_j|\theta)$
            - policy gradient with a binary reward for matching target $y_j$
   - **GopherCite: Teaching language models to support answers with verified quotes, 2022**
      - In one word: Use RLHF to train *open-book*(to search engine) QA models that generates answers while also *cite* evidence for their claims
      - Methods:
         - Inline evidence syntax(cited answer): `%&<Claim>%(Document title)%[Quote from document]%` , Self-supported Question Answering (SQA)
         - Pretraining LM, condition on on-the-shelf retrieval system: Gopher & Google
         - Training pipelines: 
            1. Collect cited answers from best current model, have it labeled by human
               - bootstrap with few-shot prompting for base Gopher model in first round
                  - (constrained)sampling tens of thousands cited answers from prompted Gopher, ask human to annotate the high-quality ones
               - collect high quality `<question, cited answer>` pairs from human
                  - a question and two candidates answers with cite
                  - check whether each answers is *Plausible* to the question and *Supported* by the quote evidence
            2. Train supervised finetuning(SFT) model, teach model to produce verbatim quotes in evidence syntax
               - only *Rated-Good*(Plausible and Supported) examples used for training
               - predict with retrieved documents and question as prompt
                  - prompt template: `{Instruction}\n [Page: {title} \n {documents}\n]+ Question: {question}\n Answer:`
               - train with uniform random number of retrieved documents to prompt Gopher within 4096 tokens limits, each document truncated with text surrounding the snippet of search engine
            3. Train a Reward Model(RM) for a scalar *overall quality* label
               - loss: average of the pairwise preference prediction loss and the Supported&Plausible response prediction loss
               - warm-start from pretrained 7B LM from Gopher family
               - RM used in inference: rerank of candidate response, SFT+top@N or RL+top@N
            4. RLFH against the Reword Model
               - maximize the reward model: $E_{P_r(x)}[r(x,y)]$
               - algorithm: A2C with KL divergence between $P_r(x)$ and initial next-tokens distribution
               - initialization: SFT model in Step-2
               - freeze 60% of layers, share parameters between policy and value function
               - introduce a *bad-syntax penalty* rules
            5. Repeat from Step-1 
         - Declining to answer 
            - use global score of Reward Model as the criteria for answer, decline if lower than a threshold
      - Tricks in high quality human annotations:
         - *super star* model with 85% aggreement on researchers in Plausible and Supported
         - attention check: further training to rater before experiments, screen out raters with too many incorrect answers
         - multiple raters: 3 for super rater pool, 6 for wider pool of raters
      - Ablation findings
         - Rerank with RM improve performance over SFT
         - RL improve performance over naive SFT or RL agent decoding with a single sample
         - In rerank regime, SFT outperforms RL in *NaturalQuestions* task
            - RL reduce the diversity of sampled answers, dimishing the benefits of reranking from large sampling
            - finetune biased to *ELI5* tasks by design
   - **Toolformer: Language models can teach themselves to use tools, 2023**
      - In one word: a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate them for the next token prediction
      - Methods: 5 steps involved
         1. Sampling API calls from LM with specific prompt $P(\textbf{x})$ for each tool
            - sampling top-$k$ positions according to $p_M(<API>|P(\textbf{x}), x_{1:i})$
            - for each position, sampling upto $m$ API calls
         2. Executes API calls: obtain $m$*$k$ response text
         3. Filtering API calls: by loss reduction threshold $T_f$
            - Loss: $L_i(\textbf{z}) = - \sum_{j=i}^{n} w_{j-i} \cdot \log p_M(x_j | \textbf{z}, \textbf{x}_{1:j-1})$
            - Loss reduction: $\min (L_i(\epsilon), L_i(e(c_i, \epsilon))) - L_i(e(c_i, r_i)) \gt T_f$ 
            - $e(c_i, r_i) := [a_c(i_c)->r]$
         4. Model finetuning: 
            - augument $\textbf{x} \in C$ with API calls: ($\textbf{x}_{1:i}, e(c_i, r_i), \textbf{x}_{i+1:n}$)
            - finetune with augumented corpus
         5. Prediction:
            - decode as usual, when generating '->', call API to fill the next token followed by ']', the continue the decoding process
      - Tools tested:
         - Question Answering:
   - **TAMER: Training an agent manually via evaluative reinforcement, 2008**
      - In one word: allows a human to train a learning agent to perform a common class of complex tasks by giving scalar reward signal for agent's observed actions
      - Methods: MDP\R paradigm
         - use supervised learning method to model human's reward
         - act greedily according to this model
         - action represented by state features changes: $\Delta f_{t+1,t} = \overrightarrow{f_{t+1,a}} - \overrightarrow{f_{t}}$
         - reward modeled by linear combination of action features
         - reward model updated online by each feedback from human
         - action with maximal current estimated reward selected (reward model act as the policy)
      - More notes
         - in domains in which human have intuition or expertise, necessary to transfer the knowledge to learning agent, by the means of judging the agent's action
         - TAMER agent greedily maximize the immediate return, leave long-term implications to human judger
         - human rewards are moving target and even personalized for human serving agent
         - human and environment reward can be combined
      - Results: a decent policy for Teris with 3 round games
   - **Deep TAMER: Interactive agent shaping in high-dimensional state spaces, 2018**
      - In one word: an extension of TAMER that leverages the representation power of DNN to learn complex tasks in a short amount of time with human trainer
      - Methods(differences to TAMER): 
         - choice of the function class to represent feedback from human
            - Pretrained Autoencoder(AE) used to project the input(image) into low dimensional space
            - 2-layers FC with 16 hidden units, one output node for each action
         - optimization algorithms to train the agent
            - decouple training updates and human feedback, a feedback replay buffer used, updating when
               - human feedback observed, compute loss for all history states within the period of feedback has affection
               - every $b$ replay buffer updates period by sampling from the replay buffer
            - weight loss by affection: $\mathcal{l}(\hat{H}; \textbf{x}, \textbf{y}) = w(t^s, t^e, t^f) (\hat{H}(\textbf{s}, \textbf{a}) - h)^2$
   - **Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization, 2022**
      - In one word: solve the problem of stability and lack of open-souce libraries and benchmarks for LM alignments by open-source library RL4LMs the GRUE benchmarks
      - RL4LMs open-source library
         - generation as a token-level MDP
         - a generic interface for per-token and per-sequence generation reward
         - On-policy actor-critic algorithms
            - value function definition: $V_t^{\pi}=E_{a_t \sim \pi}[ \sum_{\tau=t}^T \gamma^{t-\tau} R(\textbf{s}_{\tau}, a_{\tau}, \textbf{y})]$
            - Q-value function: $Q_t^{\pi}(\textbf(s)_t, a_t) = R(\textbf{s}_t, a_t, \textbf{y}) + \gamma E_{s_{t+1} \sim \pi} (V_{t+1}^{\pi} (\textbf{s}_{t+1}))$
            - advantage function: $A_t^{\pi} Q_t^{\pi}(\textbf{s}, a) - V_t^{\pi}$
            - regularized reward function: $R(\textbf{s}_t, a_t, \textbf{y}) - \beta KL(\pi_{\theta}(a_t|\textbf{s}_t || \pi_i(a_t|\textbf{s}_t)))$
      - NLPO algorithm: a *parameterized-masked extension* over PPO
         - challange: size of action space is huge, leads to instability to finetune LM with RL
         - define a mask policy $\pi_{\psi}$: 
            - select the top-p tokens from vocabulary according to $\pi_{\psi}$
            - set the prob. of remaining ones to zero when sampling actions from $\pi_{\theta}$, i.e., mask them
         - update $\pi_{\psi}$ every $\mu$ iterations
      - GRUE benchmark
         - 7 generative NLP tasks
         - task-specific mix of metrics for each task
            - task preference metrics
            - naturalness metrics
   - **Atlas: Few-shot Learning with Retrieval Augmented Language Models, 2022, MetaAI**
      - In one word: 
         - a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples
      - Investigate two questions
         - whether few-shot learning ability requies models to store a large amount of information in their parameters
         - if memorisation can be decoupled from generalization
      - Methods: language model and dense retriever
         - Retriever: a dual-decoder architecture (Contriever), pretrained using MoCo constrastive loss
         - Language model: T5 seq2seq architecture, with Fusion-in-Decoder modification
            - concatenate the outputs of encoder from different documents, perform cross-attention over the single sequence in decoder
         - Four training objectives tested
            - Attention distrillation: cross-attention scores as proxy for documents importance in LM
               - retrieval distribution: $p_{RETR}(\textbf{d}|\textbf{q})=\frac{exp(s(\textbf{d},\textbf{q})/\theta)}{\sum_k exp(s(\textbf{d}_k,\textbf{q})/\theta)}$
               - attention disbribution: $p_{ATTN}(\textbf{d}|\textbf{q})=softmax(\{AVG(\alpha_n \|v_n\|)_d\})$
               - KL distance between the above two distributions as loss
            - End-to-end training of Multi-Document Reader and Retriever: 
               - $\log [ \sum_{k=1}^K p_{LM}(\textbf{a}|\textbf{q}, \textbf{d}_k) p_{RETR(\textbf{d}_k|\textbf{q})} ]$
            - **Perplexity distrillation**: change attention distribution to
               - $p_{PERP}(\textbf{d}|\textbf{q})=softmax(\{ \log p_{LM}(\textbf{a} | \textbf{d}_k, \textbf{q}) \}_k)$
            - Leave-one-out Perplexity Distillation: complementary to perplexity distrillation
         - Three pretext(pretraining) tasks: jointly training of retriever and language modeling, initialized parameters from $\textbf{Contriever}$ and $\textbf{T5-lm-adapt}$ respectively
            - Prefix language modeling
            - Masked language modeling
            - Title to section generation
         - Efficient retriever finetuning: alleviating the need to re-computing the index whenever documents embedding updated
            - Full index update at every $R$ training steps(batches): 30% overhead compared to LM training only
            - Rerank the top $L$ documents with latest embedding index, select top $K$, 10% overhead
            - Query-side finetuning: decouple the encoding of the queries and documents
               - performance varies when large training dataset available
               - in few-shot settings, no performance degrade observed, even better
         - Datasets & experiments: 
            - Datasets: Knowledge Intensive Language Tasks/Massively-Multitask Language Understanding
            - Pretraing: pretrain for 10,000 steps, updates index every 1000 steps.
               - Datasets: 
                  - 11/20/2021 Wikipedia dump, 37M passages, 78 words in average
                  - 2020-10 Common Crawl dump, 350M passages
            - Finetuning: fixed iteration steps adapted to downstream tasks
   - **Toolformer: Language models can teach themselves to use tools, 2023**
      - In one word: a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate them for the next token prediction
      - Methods: 5 steps involved
         1. Sampling API calls from LM with specific prompt $P(\textbf{x})$ for each tool
            - sampling top-$k$ positions according to $p_M(<API>|P(\textbf{x}), x_{1:i})$
            - for each position, sampling upto $m$ API calls
         2. Executes API calls: obtain $m$*$k$ response text
         3. Filtering API calls: by loss reduction threshold $T_f$
            - Loss: $L_i(\textbf{z}) = - \sum_{j=i}^{n} w_{j-i} \cdot \log p_M(x_j | \textbf{z}, \textbf{x}_{1:j-1})$
            - Loss reduction: $\min (L_i(\epsilon), L_i(e(c_i, \epsilon))) - L_i(e(c_i, r_i)) \gt T_f$ 
            - $e(c_i, r_i) := [a_c(i_c)->r]$
         4. Model finetuning: 
            - augument $\textbf{x} \in C$ with API calls: ($\textbf{x}_{1:i}, e(c_i, r_i), \textbf{x}_{i+1:n}$)
            - finetune with augumented corpus
         5. Prediction:
            - decode as usual, when generating '->', call API to fill the next token followed by ']', the continue the decoding process
      - Tools tested:
         - Question Answering:
                              
   - **HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace, 2023, MSRA**
      - In one word: a system that leverages LLMs to connect various AI models in Huggingface to solve multi-modal AI tasks
      - Methods: four steps in all
         1. Task planning: 
            - spec. based instruction: &lt; task-id, task-type, task-deps, task-args &gt;
            - parse by filling in spec. slots by LLM
            - in-context prompt by demostrations
            - Prompts: `The AI assistant can parse user input to several tasks: [{"task": task, "id", task_id, "dep": dependency_task_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}]. The "dep" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag "<resource>-task_id" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can’t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning`
            - A demo(Use some scaffolds maybe better): `Look at /exp1.jpg, Can you tell me how many objects in the picture?\n\n [{"task": "image-to-text", "id": 0, "dep": [-1], "args": {"image": "/exp1.jpg" }}, {"task": "object-detection", "id": 0, "dep": [-1], "args": {"image": "/exp1.jpg" }}]`
         2. Model selection:
            - frame model task assignments as single choice problem, with task as question and models as choice
            - filter model based on the task-type
            - ranking model by the number of downloads, and select top-10 as choices
            - Prompts: `Given the user request and the call command, the AI assistant helps the user to select a suitable model from a list of models to process the user request. The AI assistant merely outputs the model id of the most appropriate model. The output must be in a strict JSON format: "id": "id", "reason": "your detail reason for the choice". We have a list of models for you to choose from {{ Candidate Models }}. Please select one model from the list.`
            - Candidates model: `[{"model_id": model id #1, "metadata": metadata infos #1, "description": description of model #1}, {"model_id": model id #2, "metadata": metadata infos #2, "description": description of model #2}]`
         3. Task execution:
            - Hybrid endpoint of local and HF inference
            - resources(args) referred as &lt; resouce-task_id &gt; in planning, replaced with corresponding task response during execution
         4. Response generation:
            - planned tasks, models selected for each task and the *inference results* for each model used to compose a concise summary
            - inference results in structured format with probabilites for bounding-box(obj. detect model) or answer distribution(QA model) etc.
            - LLM summarize the final response, with confidence level
            - Prompts: `With the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user’s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person.  If inference results contain a file path, must tell the user the complete file path.`
   - **Tool Learning with Foundation Models, 2023, Tsinghua**

- Multilingual & Multimodel
   - **Few-shot Learning with Multilingual Generative Language Models, 2022, Meta AI**
   - **PaLM-E: An Embodied Multimodal Language Model, 2023, Google Research**
   - **MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action, 2023, Microsoft Azure**
   - **A Generalist Agent, 2022, Google Deepmind**

- Alignments
   - **In conversation with Artificial Intelligence: aligning language models with human values, 2022, Google/DeepMind**

- DNN
   - **[Anatomize Deep Learning with Information Theory](https://lilianweng.github.io/posts/2017-09-28-information-bottleneck/)**