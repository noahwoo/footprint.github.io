---
title: Large Language Models on The Way
author: Jianmin
date: 3/2/2023
---

# 核心技术方向
- Pretrained Models
   - Left to right(GLM)
      - OpenAI: GPT/GPT-2/GPT-3
   - Masked LM
      - Google: BERT
      - Google: RoBERTa
   - Prefix LM
      - MSRA: UniLM1/UniLM2
   - Encoder/decoder
      - Google: T5
      - MSRA: MASS
      - Facebook: BART
- Prompt Engineering
   - Shape: 
      - Cloze: LAMA/TemplateNER
      - Prefix: Prefix-Tuning/Prompt-Tuning
   - Human Effort:
      - Hand crafted: LAMA/GPT3
      - Automated: 
         - Discrete: AdvTrigger/AutoPrompt
         - Continuous: Prefix-Tuning/Prompt-Tuning
- Answer Engineering

# Resources on track
- Transformers
   - **Thinking Like Transformers, 2021**
   - **Transformers: State-of-the-Art Natural Language Processing, 2020, Huggingface**
      - Targets
         - Extensible for researcher
         - Simple for practioner
         - Fast & robust for industrial deployment
      - Core modules
         - Transformers: MLM/Autoregression/Seq2Seq/Multimodal/Long-Distance/Efficient/Multilingual
         - Tokenizers: Char. Level BPE/Byte Level BPE/WordPiece/SentencePiece/Unigram/Char./Custom
         - Heads (Domain specific) : LM/Seq. Classification/QA/Token Classification(NER)/Multiple Choice/MLM/Cond. Generation
      - Deployment
         - model easy to switch from framework(say PyTorch) to the other one(say Tensorflow)
         - export to intermediate neural network format
         - use adapters to convert models to CoreML weights for edge devices

- OpenAI GPT series: 
   - GPT-1: **Improving Language Understanding by Generative Pre-Training, 2018, OpenAI** (OK)
   - GPT-2: **Language Models are Unsupervised Multitask Learners, 2019, OpenAI** (OK)
   - GPT-3: **Language Models are Few-Shot Learners, 2020, OpenAI** (OK)
   - **InstructGPT: Training language models to follow instructions with human feedback, 2022, OpenAI** (OK)
      - Reference:
         - **Fine-Tuning Language Models from Human Preferences, 2020, OpenAI**
            - Date: 3/2/2023
            - Tasks: Stylistic continuation(in sentiment or genre) and Summarization
            - Optimize over $(x, {y_i}, b)$, LogLikelihood
            - Logits consist of reward from LM $r(x,y)$ and KL dist. between new and old model
            - experiments:
               - summarization: pretrain 774M GPT-2 LM, then RL fine-tune
               - stylistic continuation: train with WebText from scrath, SFT on BookCorpus, then RL fine-tune
         - **Learning to summarize from human feedback, 2021, OpenAI**
            - Date: 3/21/2023
            - Goal: advance methods for training language models on objectives that more closely capture the behavior we care about. 
            - Task: Text summarization on Reddit TL;DR dataset
            - Annotation quality: aggreement level between labeler and researcher
               - charge by working hours instead of # of annotated samples.
               - hands-on with annotators closely
               - a detailed procedure for anotation
            - Four steps in loop
               - pre-trained model: 
                  - Corpus: C4/Webtext/books/Wikipedia
                  - 1~3 epoch on each, 300B tokens, context 2048
               - supervised model
                  - predict summary from Reddit summaries
                  - initialized from pre-trained model, cosine LR schedule, log lineear sweep
                  - batch size 128
                  - sampling with T=0 for pair-wise evaluaiton by human
                  - supervised model outperform SOTA on ROUGE score of CNN/DM dataset
               - reward model
                  - initialized from supervised model, replace the decoding matrix with a linear head to output scalar value
                  - use pair-wise loss of preferred summary to unpreferred one
               - Reinforcement Learning from Human Feedback
                  - use separated transformer parameters for policy and value networks
                  - BPE token generation as each step of episode, final reward model score as part of the RL Reward
                  - KL distance between policy and init. supervised model also included in final Reward
                  - policy network initialized with supervised model
                  - value network initialized with reward model
            - Results & Conclusions
         - **Instruct-tuning: Finetuned language models are zero-shot learners, 2022, Google** (OK)
   - Intelligence in GPT series
      - GPT-3: (davinci)
         - generation ability
         - world knowledge
         - in-context learning
      - Instruct tuning (davinci-instruct-beta)
         - instruction follow and unseen task generalization
      - Add code training (code-davinci-002)
         - code understanding
         - complex reasoning/CoT(possibly)
         - best of all: text doc and code combined
      - Supervised tuning (text-davinci-002) + RLFH (text-davinci-003)
         - ability to alignment with human
         - reject unknown/illegal/unethical question
   - ChatGPT Plugins
      - [An end-to-end 3'rd demo](https://techcommunity.microsoft.com/t5/fasttrack-for-azure/how-chatgpt-plugins-could-work/ba-p/3761483)
- Google Research/Brain/DeepMind: 
   - BERT series: 
      - **BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Google** (OK)
      - **RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2019, Facebook** (OK)
   - LaMDA series: (Language model for Dialog Application)
      - **Towards a Human-like Open-Domain Chatbot, 2020, Google**
      - **Finetuned language models are zero-shot learners, 2022, Google** (LaMDA-PT -> FLAN) (OK)
         - instruct tuning for multi-task generalization, with 60+ NLP tasks
      - **Scaling Instruction-Finetuned Language Models, 2022, Google** (Flan series: T5/PaLM) (OK)
         - scaling instruction tasks to 1.8k 
         - scaling model size from 80M to 540B
         - fine-tuning on CoT data
      - **The Flan Collection: Designing Data and Methods for Effective Instruction Tuning, 2023, Google**
   - GLaM series: (More Efficient In-Context Learning, sparse language model(FFN -> MoE))
      - **Efficient Scaling of Language Models with Mixture-of-Experts, 2022, Google**
   - PaLM series: (540-Billion parameters, attention and ffn computed in parallel)
      - **PaLM: Scaling Language Modeling with Pathways, 2022, Google, Jeff Dean**
   - Chinchilla: (optimal model size and #tokens for training a transformer language model under a given compute budget)
      - **Training Compute-Optimal Large Language Models, 2022, (Google/DeepMind)**
   - Gopher: (an analysis of Transformer-based language model performance across a wide range of model scale)
      - **Scaling Language Models: Methods, Analysis & Insights from Training Gopher, 2022, (Google/DeepMind)**
   - T5 series: (training all kinds of task in unified text-to-text way)
      - **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, 2020, Google (T5)**
   
- RLFH series:
   - **Illustrating Reinforcement Learning from Human Feedback (RLHF), 2022, Hugging Face Blog**
      - [Github Link](https://github.com/huggingface/blog/blob/main/rlhf.md)

- Prompt(hard/soft) engineering
   - Survey:
      - **Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing, 2021, CMU** (OK)
      - **[Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/), 2023, OpenAI**
   - Parameter efficient tuning
      - **Prefix Tuning: Optimizing Continuous Prompts for Generation, 2021, Stanford** (OK)
      - **Prompt Tuning: The Power of Scale for Parameter-Efficient Prompt Tuning, 2021, Google** (OK)
      - **P-tuning/P-tuning V2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks, 2021, Tsinghua** (OK)
      - **Unified View: Towards a unified view of parameter-efficient transfer learning, 2022, CMU** (OK)
         - One-word: Unify Adpater/LoRA/Prefix-tuning into the modification to specific hidden states(heads) in pretrained model
         - Aspects of modifications
            - target: head-attention(Prefix Tuning), attention(Adapter/LoRA), ffn(Adapter), key/value tranform matrix(LoRA)
            - composition: $h \leftarrow h + s \Delta h$ or $h \leftarrow (1-\lambda(x)) h + \lambda(x) \Delta h$ (PrefixTuning)
            - modifier $\Delta h$: 
               - low rank bottleneck: $f(v W_1)W_2, W_1 \in R^{d \times l}, W_2 \in R^{l \times (d|d_h)}$
               - $v$: PLM layer input $x$(Prefix Tuning/LoRA) or (head-)attention $h$(Adapter) 
               - $f$: Identity mapping(LoRA), ReLU activation function(Adapter), Softmax function(Prefix Tuning)
               - parallel(PrefixTuning/LoRA) or sequential(Adapter)
               - scaling or not: yes(LoRA), no(Prefix Tuning/Adapter)
         - Results
            - Claimed comparable performance to fine-tuning not generalize well to other benchmark
            - parallel adapter beats sequential adapter
            - ffn modification utilize the added parameters more effectively than (head-)attention, except for the case with less than 0.1% parameters added
            - scaling composition function better than vanilla additive one
            - Mix-And-Match adapter utilizing the good of Prefix-tuning and Adapter works better
   - Automation:
      - **Large Language Models Are Human-Level Prompt Engineers**, 2023
   - Applications: 
      - **Prompt tuning GPT-2 language model for parameter-efficient domain adaptation of ASR systems, 2022, Amazon** (OK)

- In context learning
   - Survey
      - **A Survey on In-context Learning, 2023, PKU** (OK)
         - 3/3/2023: read through
   - Explanations:
      - **Rethinking the Role of Demonstrations: What Makes In-Context LearningWork?, 2022, Facebook**
      - **An Explanation of In-context Learning as Implicit Bayesian Inference, 2022, Stanford**
      - **[How does in-context learning work? A framework for understanding the differences from traditional supervised learning](https://ai.stanford.edu/blog/understanding-incontext/), 2022, Stanford**
         - One word: 
            - In context learning as a Bayesian inference of the prompt concept that every example in the prompt shares: $z$ in $P(z|p)$
         - Methods
            - Pretraining distribution: assume LLM fits the pretraining distribution exactly
            - Prompt distribution: In context prompt examples are drawn from the same prompt concept
            - Bayesian inference for in context learning formally:
               - $P(o|p) = \int_{z} P(o|p,z) P(z|p) dz$, $p$ for prompts, $z$ for latent concept, $o$ output of LLM
               - $P(z|p)$ concentrates on the prompt concept with more examples in the prompt 
               - Noise and signal
                  - Training prompt examples privide signal: Strong enough to overwrite the noise
                  - Transition between examples in low prob.: unnatural text, different from the pretraining distribution
                  - In-context learning robust to noise
            - Empirical evidence
               - Forming the prompt with the ground truth output is not required to achieve good in-context learning performance
               - The underlying input distribution that examples are drawn matters
               - The set of outputs in the task and input-output format matter
               - One more evidence: In-context learning performance is highly correlated with term frequencies during pretraining
            - Avenues for extensions
               - Input-output ground truth mapping matters for synthetic tasks
               - Instruction as improving Bayesian inference by providing explicit observations of the latent prompt concept
               - Preference of pretraining data for eliciting in-context learning
      - **What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, 2023, Stanford**
      - **Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers, 2022, Tsinghua**

- CoT/Reasons: 
   - **Chain of thought prompting elicits reasoning in large language models, 2022, Google** (OK)
      - Prompt with chain of thought helps improve perf. on arithmetric, commonsense and symbolic reasoning
      - Ablation study shows: 
         - equation only prompting not work
         - expressing intermediate steps via natual language helps
         - sequential reasoning embodied in chain of thought is useful for reasons beyond just activating knowledge
   - **Self-consistency improves chain of thought reasoning in language models, 2023, Google**
   - **Reasoning with Language Model Prompting: A Survey, 2022, Alibaba**
      - One word: survey of cutting-edge research on reasoning with language model prompting
      - Methods
         - Strategy Enhenced Reasoning: design better reasoning strategy, like CoT/Multi-stage CoT
            - Prompt Enginneering: 
               - Single stage: 
                  - CoT works; more Chain steps, better results
                  - Exemplars' quality, diversity and order matters
                  - Even zero-shot works by prompting with 'Let's think step by step'
               - Multi stage: decompose into subtasks, remedy compositionality gap
                  - Reasoning step by step in sequential, concate subtask question to the end of prompt
                  - Split and merge tasks with different prompts
            - Process optimization: Natural language rationales
               - Reasoning consistency and continuity matters
               - Self Optimization
               - Iterative Optimization
                  - StAR: a small set of exemplars to generate reasoning steps, ones lead to correct answer used for fine-tuning
                  - generates multiple reasoning processes, the most consistent ones used for fine-tuning
                     - Selfconsistency improves chain of thought reasoning in language models, 2022, Google
               - Ensemble Optimizization
                  - not all reasoning steps should undertake the incorrectness of answers
                  - sampling multiple reasoning processes, generate most consistant answer by vote
            - External engine: 
               - 2 goals in LLM: semantic understanding and complex reasoning, first one can leverage external system
               - examples of external engine(no world knowledge, just rules)
                  - Physical simulator: 
                  - Code interpreters: PAL/POT
         - Knowledge Enhenced Reasoning
            - Implict knowledge: elicit knowledge from LM by few-shot prompting, then prompot downstream LM
            - Explict knowledge: retrieval augumented LM
               - Learning to retrieve prompts for in-context learning, 2022
               - Rainier: Reinforced knowledge introspector for commonsense question answering, 2022
      - Tasks
         - Arithmetic/Commonsense/Symbolic/Logical/Multimodel reasoning
         - Visualcomet: Reasoning about the dynamic context of a still image, 2020 (GPT-4 image reasoning demo)
   - **Towards Reasoning in Large Language Models: A Survey, 2022, UIUC**
   - **ReAct: Synergizing reasoning and action in language models, 2022, Google** (OK)
   - **Show Your Work: Scratchpads for Intermediate Computation with Language Models, 2021, Google Brain**
      - One word: fine-tune Transformers to perform multi-step computations(long digits addition/program exec.) with intermediate computation steps as prompt written into a “scratchpad”
      - Conclusions: 
         - encoding long digits addition process step by step (add&carry) as text and doing supervised fine-tuning improve the performance of long digits addition tasks
         - encoding polynomial evaluation process step by step (item by item) as text and fine-tuning improve the performance of polynomial evaluation tasks
         - emitting full program traces line by line annotated with local variables improve the performance of computer program execution prediction
      - Method & Experiments
         - Use decoder-only transformer models, pretrained model ranging in size from 2M to 137B parameters
         - Addition task
            - Train with 1-8 digits addition task ,test with 1-8 in-distribution and 9-10 out-of-distribution tasks
            - Fine-tuning with 100K examples for 5K steps, batch size 32
            - Test with 10K in-distribution and 1K out-of-distribution examples
            - Scratchpad fine-tuning models perform well with the scaling of model size

   - **StAR: Bootstrapping reasoning with reasoning, 2022, Google**
      - One word: iteratively leverage a small number of rationale examples and a large dataset without rationales, to bootstrap the ability to perform more complex reasoning
      - Methods:
         - In brief:
            1. prompt with few-shot rationale examples, answer questions
            2. if answer wrong, generate rationale with correct answer (Rationalization)
            3. fine-tuning on <P, rationale, C> with rationale leading to or rationalized with correct answer from step 1 and step 2
            4. repeat from step 1 (hence the 'bootstrap')
         - In details: 
            - almost the same as in brief
            - a view in RL: $J(M,X,Y) = \sum_i \mathcal{E}_{\hat{r_i}, \hat{y_i} \sim P_M(.|x_i)} \mathcal{1}(y_i=\hat{y}_i)$
            - Rationalization implemented by mark the correct answer with '(CORRECT)' in prompt
         - Experiments:
            - Arithmetic: n-digits sum with scratchpad
            - CommonsenseQA: 12k question with five choices
            - GSM8K: grade-school-level word problem
   - **Self-ask: Measuring and narrowing the compositionality gap in language models, 2022, MetaAI**
      - One word: measure and solve the problem of compositional gap: the fraction of incorrectly answered questions with correct sub-problems answers by model
      - Methods: CoT and self-ask works, self-ask perform better with the help of search engine
         - In breif: Similar to least-to-most, prompt with examples
            - started with 'Are follow up questions needed here: Yes/No'
            - followed with Intermediate Question and Answer from LM or Search Engine
            - repeat until all the follow-up question answered
            - 'So the final answer is:' followed up with final answer
         - Details:
            - measure the compositionality gap:
               - a model can compose facts at a much higher rate when it can recall these facts more confidently
            - CoT & Self-ask
               - CoT and self-ask let the model apply more computation to harder problems by demonstrating the reasoning chain
            - Employ search engine for realtime world knowledge
               - answer the follow-up questions by search engine(feature snippets or snippets from top ranked result)
      - Experiments & datasets
         - Compositional Celebrities dataset
         - 2WikiMultiHopQA
         - Bamboogle: a dataset of 125 questions
            - reading random Wikipedia articles and writing a 2-hop question about them
            - fitler the questions that can be answered by search engine correctly
   - **Least-to-most prompting enables complex reasoning in large language models, 2022, Google**
      - Oneword: Solve problem harder than the demonstration examples in CoT prompt
      - Methods: least to most prompting, using a progressive sequence prompts to help language model learn a new skill
         - In brief: based on few-shot prompting
            - reduce the complex problem in many sub-problems by querying LLM
            - each sub-problem solved sequentially by querying LLM, with answers from the previous sub-problem as facilitating prompts
         - In details
            - Stage 1: reduce to subproblems: prompt LM with examples of how to reduce the complex problem into subproblems, follow by a specific problem to reduce
            - Stage 2: solve subproblems sequentially, with complex problem appended as the last one
               - prompt LM with examples of how the first subproblem solved, followed by the first subproblem
               - take the answer from LM, append it to the previous prompts, followed by the next subproblem
               - continue util reach the last subproblem, i.e., the complex problem, return the answer then
         - Experiments: Output zero-shot and CoT in all datasets
            - Symbolic manipulations: last-letter-concatenation
            - Compositional generalization: SCAN
            - Math reasoning: DROP & GSM8K
   - **Multimodal chain-ofthought reasoning in language models, 2023, Amazon(Alex Smola)**
   - Automatic CoT
      - **Automatic Prompt Augmentation and Selection with Chain-of-Thought from Labeled Data, 2023**

- World knowledge and augumented language model
   - **REALM: Retrieval-Augmented Language Model Pre-Training, 2020, Google** (OK)
      - Neural retrieval augumented LM generator: P(y|x) = \sum_z P(z|x) P(y|x,z)
      - P(z|x) a transformer encoded retrieval with vector similarity as the ranking criteria
      - Marginalize on z over Top-K approximation to reduce the computation burden
   - **In-Context Retrieval-Augmented Language Models, 2020, AI21 Labs** (OK)
   - **RAG: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, 2021, Facebook** (OK)
      - Similar idea as REALM
      - Seq2seq language model generator: BART
      - Neural retrieval: dense passage retrieval(DPR) for Wikipedia documents, with two transformer encoders for query and doc
      - Document encoder fixed to pretrained bi-encoder DPR model on TriviaQA and Natural Questions
      - Learned retrieval ablation tested, show performance gain
      - Hotspot index swap for knowledge update works
   - **MRKL(miracle): Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning, 2022, AI21** (OK)
      - Call for neural and symbolic combined approach, to overcome the limitation of neural language model
      - Arithmetic as a test case, focus on the neural router to extract operands and operators from arithmetical question
      - Templates are used to generate <prompt, completion> pairs for fine tuning of neural router
      - Prompt tuning employed for soft-prompt in experiments
   - **RETRO: Improving language models by retrieving from trillions of tokens, 2022, Google/DeepMind**
   - **WebGPT: Browser-assisted question-answering with human feedback, 2022, OpenAI**
      - In brief
         - fine-tuned on question-human demonstration pairs
         - optimization via RLHF
      - Method on details, four aspects
         1. behavior cloneing(SFT/Bahavior Cloning)
         2. reward modeling(RM)
         3. reinforcement learning(RLFH)
         4. reject sampling (an alternative to #3, in exchange of offline training with online inference)
      - Results
         - ELI5 dataset: 56% preferred over people, 69% preferred over top-voted by ELI5 dataset
         - TruthfulQA dataset: truth 75% of the time, truth and informative 54% of the time
      - Ref. 
         - Learning to summarize from human feedback, 2020, OpenAI
   - **DSP: Composing retrieval and language models for knowledge-intensive NLP, 2023, Stanford**
   - **Knowledge Retrieval Architecture for LLM’s** (OK)
      - [Web Link](https://mattboegner.com/knowledge-retrieval-architecture-for-llms/)
   - **Augmented Language Models: a Survey, 2023, Facebook** (OK)
      - Key elements (Langchain abstract as Agent & Tools)
         - Reasoning: given more computation steps to the model before yielding the answer to a prompt
         - Tool: getting each step right
         - Action: call tool and observe the result
      - Augument LM's reasoning capability
      - Allow LM to interact with exteral tools and act
      - Reasoning and tools usage implemented by
         - Supervision
            - Few shot prompting
            - Gradident-based learning(instruct tuning mostly used)
               - Blender Bot
               - WebGPT
            - Prompt pre-training: mix pre-training data with labeled demo. of reasons
               - Galactica
            - Bootstrapping: prompt LM to reason or act in few shot setup with final prediction, examples lead to incorrect prediction removed, initial LM fine-tuned on coorect examples 
               - **STaR: Self-taught reasoner bootstrapping reasoning with reasoning, 2022**
               - **Talm: Tool augmented language models, 2022**
                  - Oneword: text-only approach to augument LM with non-differential tools, self-play technique to bootstrap performance with few tool usage demonstrations at the beginning
                  - Methods:
                     - Task and tool interface: *task input text |tool-call too input text |result tool output text |output task output text*
                     - Self-play: 
                        1. start with bootstrap set $D = \{(x_j, t_j, r_j, y_j)\}$
                        2. finetune LM with $D$
                        3. for each examples in task, sample tool & input($t_j$), call tool to get the result, sample task output
                           - use high temperature(1.0) and sample size (topk=40) to explore text of diverse tool API invocation 
                        4. if task output match target $y_j$ within threshold, add $(x_j, t_j, r_j, y_j)$ to D
                        5. goto step 2
                     - RL analogy:
                        - policy-gradient with LM as policy $\pi(t_j|\theta)$
                        - policy gradient with a binary reward for matching target $y_j$
         - Reinforcement Learning
            - Human preference data(ranking/like/dislike): from SFT to RL
            - Most RL work teach LM to act rather than reason(besides STaR)
            - Hardcode reward:
               - **Conqrr: Conversational query rewriting for retrieval with reinforcement learning, 2022**
               - **Rainier: Reinforced knowledge introspector for commonsense question answering, 2022**
               - **Webshop: Towards scalable real-world web interaction with grounded language agents, 2022**
               - **Regen: Reinforcement learning for text and knowledge base generation using pretrained language models, 2021**
            - Human feedback(RLFH): Alignment:
               - **Tamer: Training an agent manually via evaluative reinforcement, 2008**
               - **Deep tamer: Interactive agent shaping in high-dimensional state spaces, 2018**
               - **Is reinforcement learning (not) for natural language processing?: Benchmarks, baselines, and building blocks for natural language policy optimization, 2022**
               - InstructGPT
               - WebGPT 
               - **GopherCite: Teaching language models to support answers with verified quotes, 2022**
               - diff. in external module vs. external tools(web browser) ??
               - **Toolformer: Language models can teach themselves to use tools, 2023**
                  - Oneword: a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate them for the next token prediction
                  - Methods: 5 steps involved
                     1. Sampling API calls from LM with specific prompt $P(\textbf{x})$ for each tool
                        - sampling top-$k$ positions according to $p_M(<API>|P(\textbf{x}), x_{1:i})$
                        - for each position, sampling upto $m$ API calls
                     2. Executes API calls: obtain $m$*$k$ response text
                     3. Filtering API calls: by loss reduction threshold $T_f$
                        - Loss: $L_i(\textbf{z}) = - \sum_{j=i}^{n} w_{j-i} \cdot \log p_M(x_j | \textbf{z}, \textbf{x}_{1:j-1})$
                        - Loss reduction: $\min (L_i(\epsilon), L_i(e(c_i, \epsilon))) - L_i(e(c_i, r_i)) \gt T_f$ 
                        - $e(c_i, r_i) := [a_c(i_c)->r]$
                     4. Model finetuning: 
                        - augument $\textbf{x} \in C$ with API calls: ($\textbf{x}_{1:i}, e(c_i, r_i), \textbf{x}_{i+1:n}$)
                        - finetune with augumented corpus
                     5. Prediction:
                        - decode as usual, when generating '->', call API to fill the next token followed by ']', the continue the decoding process
                  - Tools tested:
                     - Question Answering:
                        - **Atlas: Few-shot Learning with Retrieval Augmented Language Models, 2022, MetaAI**
                     - Calculator
                     - Wikipedia Search: BM25 retrieval on the indexes of Wikepedia dump
                     - Machine Translation System
                     - Calendar
      - Discussion
         - **Looped transformers as programmable computers, 2023**
         - **A path towards autonomous machine intelligence, 2022, Lecun**
         - **Language models (mostly) know what they know, 2022**
         - **React: Synergizing reasoning and acting in language models, 2022**

- Alignments
   - **In conversation with Artificial Intelligence: aligning language models with human values, 2022, Google/DeepMind**